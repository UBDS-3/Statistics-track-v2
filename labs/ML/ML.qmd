---
title: 'Machine learning'
author: "Bio Data Science^3 faculty"
date: "July 19 - August 2, 2025"
format:
  html:
    code-fold: false
    code-tools: true
    embed-resources: true
    highlight-style: github
    toc: true
    code-line-numbers: false
params:
  answers:    true
  simplified: true
  sessioninfo: false
---

```{r}
#| label: initialize
#| echo: false
#| include: false
options("scipen"=100, "digits"=4)
knitr::opts_chunk$set(
    eval  = params$answers,
    echo  = params$answers,
    message = FALSE,
    warning = FALSE,
    fig.width=6, fig.height=4, fig.align="center", fig.pos="h"
)
set.seed(533)
```

## Goal

In this lab we will learn the basics of Machine Learning (ML). We will focus
on supervised learning. We will start with the simple idea of linear
discrimination, and then move on to important concepts in ML: cross-validation,
overfitting and variance-bias trade-off.

## Packages

```{r}
#| label: loadPkgs
#| eval: true
#| echo: true
# load required packages
library(MASS)
library(ExperimentHub)
library(tidyverse)
library(glmnet)
library(RColorBrewer)
library(caret)
```

## Diabetes data set

The `diabetes` dataset presents three different groups of diabetes patients and
five clinical variables measured on them. We will try to use those measured
variables to predict the group of a patient.

```{r}
#| label: diabetes
#| eval: false
#| echo: true
# load data
diabetes = read_csv(
  url("http://web.stanford.edu/class/bios221/data/diabetes.csv"),
  col_names = TRUE
)
# if download fails with timeout error, try increasing it
# options(timeout = 1000)
diabetes
```

```{r}
#| label: diabetesLocal
#| echo: true
#| eval: true
diabetes = read_csv("diabetes.csv")
diabetes
```
We convert values of `group` column from character into factor. If you are not
familiar with factors in R, you can read about them
[here](https://r4ds.hadley.nz/factors.html).

```{r}
#| label: factor
#| eval: true
#| echo: true
# convert group to factor
diabetes$group <- factor(diabetes$group)
```

Next we will visualize the data using `ggplot2` package. To plot all variables
together, we need to transform the data into a long format, using `gather`
function from `tidyr` package.

```{r}
#| label: ldagroups
#| eval: true
#| echo: true

# transform data to long format
diabetes.long = pivot_longer(
  data = diabetes,
  cols = -c(id, group),
  names_to = "variable",
  values_to = "value"
)

# plot distribution of different variables in the data
ggplot(data = diabetes.long, mapping = aes(x = value, col = group)) +
  geom_density() + facet_wrap( ~ variable, ncol = 2, scales = "free")
```

We see already from the one-dimensional distributions that some of the
individual variables could potentially predict which group a patient is more
likely to belong to.

::: {.callout-note collapse="false"}

## Question

Which of the variables seem to be most informative for distinguishing the groups?

```{r}
#| label: informativeSolution
cat("Insulin")
```

:::

Our goal will be to combine variables to get the more accurate predictions.

## Linear discrimination analysis (LDA)

We start with one of the simplest possible discrimination problems.

We will use two variables, `insulin` and `glutest`, to predict the `group`.
Our aim is to partition the 2D plane defined by these variables into classes,
using class boundaries that are straight lines.

::: {.callout-note collapse="false"}

## Question

It's always a good idea to first visualize the data. Look at how `insulin` and
`glutest` are distributed relative to each other by plotting the scatterplot
of the two variables and colouring the points by the group.

```{r}
#| label: scatterdiabetesHint
#| eval: false
#| echo: !expr params$simplified
# Fill in the blanks to complete the code and answer the question above

ggdb <- ______(________, aes(x = _______, y = _______, colour = _____)) +
         geom_point()

# Hint:
# ...
```

```{r}
#| label: scatterdiabetesCodeSolution
#| eval: true
#| echo: false
ggdb <- ggplot(diabetes, aes(x = insulin, y = glutest, colour = group)) +
         geom_point()
```

```{r}
#| label: scatterdiabetesPlotSolution
#| fig.width: 5
#| fig.height: 4
ggdb <- ggplot(diabetes, aes(x = insulin, y = glutest, colour = group)) +
         geom_point()
ggdb
```

:::

We'll start with a method called linear discriminant analysis (LDA).
This method is a foundation stone of classification, many of the more
complicated (and sometimes more powerful) algorithms are really just
generalizations of LDA.

To calculate LDA, we will use the `lda` function from the `MASS` package.
One way to run this function (and many other models in R) is using the formula
notation. To read more about LDA and formula format, check `?lda` and `?formula`.

In the simple form, a formula can look like this:

`response ~ predictor1 + predictor2 + ... + predictorn`

Here `response` is the variable we want to predict, and `predictor1`,
`predictor2`, etc. are the variables we use to make the prediction.

Response and predictors can be vectors of data used in the model.
Alternatively, if data argument is specified, they can be (unquoted!) column
names of the data that contain the response and predictor values.
Yet another alternative is to fit `lda` by providing as parameter `x` a data.frame
of predictor variables (with observations as rows and variables as columns), and
as parameter `groupping` a corresponding factor of response variables i.e. groups.

Below we show these different ways in which we can fit the LDA model using
diabetes data. All the three lines of code are equivalent.

```{r}
#| label: ldafit
#| eval: true
#| echo: true
# three ways to fit LDA model:
diabetes_lda <- lda(diabetes$group ~ diabetes$insulin + diabetes$glutest)
diabetes_lda <- lda(group ~ insulin + glutest, data = diabetes)
diabetes_lda <- lda(x = diabetes[, c("insulin", "glutest")], grouping = diabetes$group)

# inspect the model
diabetes_lda
```

In the shown model summary:
- prior probabilities are calculated as the fraction of samples from every group
- group means are the means of the variables for each group
- coefficients are the coefficients of the linear model
- proportion of trace are the proportions of inter-group variance explained

We can then use the model for prediction.

```{r}
#| label: ldaResults
#| eval: true
#| echo: true
# predictions from model
ghat <- predict(diabetes_lda)$class

# contingency table
table(predicted = ghat, truth = diabetes$group)

# prediction error
mean(ghat != diabetes$group)
```

Now, let's visualize the LDA result. We are going to plot the prediction regions
for each of the three groups. We do this by creating a grid of points and using
our prediction rule on each of them (note the parameter `newdata` in the
`predict()` function now) On top of this, we also plot the class centers.
Assembling this visualization requires us to write a bit of code, but don't
worry too much if you don't understand it - we do this just to illustrate the
concepts.

```{r}
#| label: plotLdaResults
#| eval: true
#| echo: true
#| fig.width: 5
#| fig.height: 4

# helper function to make 1D grid
make1Dgrid <- function(x) {
  # extend the range of x by small fraction on both sides
  rg <- grDevices::extendrange(x)
  # generate 100 sequential values within extended range of x
  seq(from = rg[1], to = rg[2], length.out = 100)
}

# get all combinations of insulin and glutest in the range of the data
diabetes_grid <- with(
  diabetes,
  expand.grid(
    insulin = make1Dgrid(insulin),
    glutest = make1Dgrid(glutest)
  )
)

# predictions for the grid values
diabetes_grid$ghat <- predict(diabetes_lda, newdata = diabetes_grid)$class

# group centers
centers <- diabetes_lda$means

# plotting on top of the diabetes data scatterplot we generated before
ggdb +
  # we add prediction regions plotted as raster image on top of original points
  geom_raster(
    data = diabetes_grid,
    mapping = aes(x = insulin, y = glutest, fill = ghat),
    inherit.aes = FALSE, alpha = 0.25, interpolate = TRUE
  ) +
  # we add centers as + points
  geom_point(
    data = as_tibble(centers),
    mapping = aes(x = insulin, y = glutest),
    inherit.aes = FALSE, pch = "+", size = 8
  ) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))
```

::: {.callout-note collapse="false"}

## Question

Instead of trying to predict with 2 variables, fit LDA model with all 5 variables.
Get predictions using this new model.
Get the confusion table for predictions, and calculate the prediction error.

```{r}
#| label: lda5Hint
#| eval: false
#| echo: !expr params$simplified
# Fill in the blanks to complete the code and answer the question above

diabetes_lda5 <- ___(_____ ~ _____ + _______ + _______ + ______ + _______, data = ________)

# predicitons
ghat5 <- _______(diabetes_lda5)$_____

# confusion table
_____(ghat5, diabetes$_____)

# error
____(ghat5 != diabetes$_____)

# Hint:
# ...
```

```{r}
#| label: lda5Solution

# fit the model
diabetes_lda5 <- lda(group ~ relwt + glufast + glutest + steady + insulin, data = diabetes)

# predicitons
ghat5 <- predict(diabetes_lda5)$class

# confusion table
table(ghat5, diabetes$group)

# error
mean(ghat5 != diabetes$group)
```

How did the prediction error change compared to the model with only 2 variables?

```{r}
#| label: lda5ErrorSolution
#| echo: false
#| results: 'asis'
cat(
  "It decreased - the prediction error is almost half compared to what we got with only the insulin and glutest variables."
)
```

Based on this, can you tell if the classifier trained with 5 variables is better
than the one trained with only 2?

```{r}
#| label: ldaTrainComparisonSolution
#| echo: false
#| results: 'asis'
cat(
  "No, because the errors are computed on the same data that we use for model training, so the model with lower error might actually be overfitting the data."
)
```

:::

## Cross-validation

Previously we were looking at predictions and classification error on the same
data that we used for training the model. This is not the most correct approach,
as the model will always perform better on the training data than on the new,
unseen data. In extreme cases, the model could fail to learn the trends
underlying the data, but instead it simply memorizes the values that it is
trained on - this is called overfitting, and is something we want to avoid.

To get a good idea if the model learns well, we need to estimate its performance
on new data it hasn't seen before. The most simple way to do this, we would to
use a fraction of data only for training, and a fraction only for testing.
However, by splitting the data only once, our results can be quite affected by
how the split happens to fall, especcially if we don't have many observations.
Another way to deal with this would be to perform cross validation (CV).

Below we will look at one version of CV, called leave-one-out cross-validation
(LOOCV). In LOOCV, we fit the whole model without one data point and then we
predict the label of the left-out point. We know the true label of that data
point, and the learning algorithm does not, hence we can get a reasonable
estimate of the algorithm's performance on this "new" data.  Then, this process
is repeated for each data point separately, and the average error is calculated.

::: {.callout-note collapse="false"}

## Question

Let's write a function that fits `lda` model with LOOCV.
In the code below, the parameters of the function are described after `@param`
keywords, and the `@value` describes what the function should output.
Complete the function template provided (replace `...` with code).

```{r}
#| label: estimateMclLoocvSolution
#| eval: true

#' A function to perform LOOCV
#'
#' @param x data.frame of predictor variables (rows are observations, columns are variables)
#' @param grouping factor, response variable (length should be the same as nrow(x))
#'
#' @value vector with mean training and test error
#'
estimate_mcl_loocv = function(x, grouping) {
  # iterate over observations
  vapply(seq_len(nrow(x)), function(i) {
    # fit a model with i-th observation left-out
    fit  = lda(x = x[-i, ], grouping = grouping[-i])
    # predict on the training data
    ptrn = predict(fit, newdata = x[-i,, drop = FALSE])$class
    # predict on the test data i,e, left-out i-th observation
    ptst = predict(fit, newdata = x[ i,, drop = FALSE])$class
    # return training and test errors
    c(
      train = mean(ptrn != grouping[-i]),
      test = (ptst != grouping[i])
    )
  }, FUN.VALUE = numeric(2)) %>% rowMeans
}
```

```{r}
#| label: estimateMclLoocvTemplate
#| eval: false
#| echo: true

#' A function to perform LOOCV
#'
#' @param x data.frame of predictor variables (rows are observations, columns are variables)
#' @param grouping factor, response variable (length should be the same as nrow(x))
#'
#' @value vector with mean training and test error
#'
estimate_mcl_loocv = function(x, grouping) {
  # iterate over observations
  vapply(seq_len(nrow(x)), function(i) {
    # fit a model with i-th observation left-out
    fit  = ...
    # predict on the training data
    ptrn = ...
    # predict on the test data i,e, left-out i-th observation
    ptst = ...
    # return training and test errors
    c(
      train = ...,
      test = ...
    )
  }, FUN.VALUE = numeric(2)) %>% rowMeans
}
```

:::

::: {.callout-note collapse="false"}

## Question

Now compare again the errors for model trained with only `insulin` and `glutest`,
and the model trained with all variables. Which model is better?

```{r}
#| label: ldaLoocvHint
#| eval: false
#| echo: !expr params$simplified
# Fill in the blanks to complete the code and answer the question above

estimate_mcl_loocv(x = ________[, c("_______", "_______")], grouping = diabetes$_____)
estimate_mcl_loocv(x = ________[, c("_______", "_______", "_______", "_____", "______")], grouping = diabetes$_____)

# Hint:
# ...
```

```{r}
#| label: ldaLoocvSolution
estimate_mcl_loocv(x = diabetes[, c("insulin", "glutest")], grouping = diabetes$group)
estimate_mcl_loocv(x = diabetes[, c("insulin", "glutest", "glufast", "relwt", "steady")], grouping = diabetes$group)
```

```{r}
#| label: ldaLoocvErrorSoltuion
#| echo: false
#| results: 'asis'
cat(
  "Both the training and the test error are lower for the model with 5 variables"
)
```

:::

In the section below you will also see CV used to pick a parameter for LASSO-logistic
regression.

## Metagenome data set

Zeller et al. (2014) studied metagenome sequencing data from fecal samples of
156 humans that included colorectal cancer patients and tumor-free controls.
Their aim was to see whether they could identify biomarkers (presence or
abundance of certain taxa) that could help with early tumor detection. The data
are available from [Bioconductor](https://www.bioconductor.org) through its
`ExperimentHub` service under the identifier EH359.

```{r}
#| label: colon1
#| eval: true
#| echo: true

library(ExperimentHub)

# get experiments
eh = ExperimentHub()
# if you encounter problem, try running
# eh = ExperimentHub(localHub = TRUE)

# selected experiment
zeller = eh[["EH361"]]

# inspect ExpressionSet object
zeller
```

`ExpressionSet` contains gene expression data, together with the samples and features information.
Expression data can be accessed with `exprs()`, sample information with `pData()` (phenotypic data), and feature information with the `fData()` accessors.

Check sample information for `zeller` dataset.

```{r}
#| label: zeller_pData
#| eval: true
#| echo: true
head(pData(zeller))
tail(pData(zeller))
```

Columns of the annotations can be accessed directly by their names, e.g. `zeller$disease`.

```{r}
#| label: zeller_pData_disease
#| eval: true
#| echo: true
table(zeller$disease)
```

For the following exercises, we will focus only on the normal (`n`) and `cancer` samples, so we will now also sample the `ExpressionSet` object to include only these samples.

```{r}
#| label: zeller_subset
#| eval: true
#| echo: true
zellerNC <- zeller[, zeller$disease %in% c("n", "cancer")]
```

Notice that feature information in this dataset (`fData(zellerNC)`) is empty.
We only have feature names in the expression data.

```{r}
#| label: zellerpData_end
#| eval: true
#| echo: true
head(rownames(zellerNC))
tail(rownames(zellerNC))
```

As you can see, the features are a mixture of abundance quantifications at
different taxonomic levels, from **k**ingdom over **p**hylum to **s**pecies.
We could select only some of these, but here we continue with all of them.

Next, let's look at the distribution of some of the features. Here, we show
an arbitrary choice of features; in practice, it is helpful to scroll through
many such plots quickly to get an impression.

```{r}
#| label: zellerHist
#| eval: true
#| echo: true

# convert to data.frame
tidy_zeller <- as.data.frame(t(exprs(zellerNC)))

# select features to show
feats <- colnames(tidy_zeller)[1:4]

# histograms for selected features
for (i in feats) {
  hist(tidy_zeller[[i]], main = i, xlab = "Abundance")
}

```

## GLM with L1 regularisation (LASSO)

In the simplest case, we fit model

$$
\log \frac{P(Y=i\,|\,X=x)}{P(Y=k\,|\,X=x)} = \beta^0_i + \beta_i^\top x
$$

as follows.

```{r}
#| label: glmnet
#| eval: true
#| echo: true
library("glmnet")
glmfit = glmnet(
  x = t(exprs(zellerNC)),
  y = factor(zellerNC$disease),
  family = "binomial"
)
```

A remarkable feature of the `glmnet` function is that it fits the model not
only for one choice of $\lambda$, but for all possible $\lambda$s at once.
For now, let's look at the prediction performance for, say, $\lambda=0.04$.
The name of the function parameter is `s`:

```{r}
#| label: colonPred
#| eval: true
#| echo: true
pred = predict(glmfit, newx = t(exprs(zellerNC)), type = "class", s = 0.04)
confusion_table = table(predicted = pred, truth = zellerNC$disease)
confusion_table
```

::: {.callout-note collapse="false"}

## Question

What is the true positive rate (TPR, i.e. sensitivity) and true negative rate (TNR, i.e. specificity)?
Hint: See [Wikipedia](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).

```{r}
#| label: tprTnrHint
#| eval: false
#| echo: !expr params$simplified
# Fill in the blanks to complete the code and answer the question above

# TPR
confusion_table[___]/(confusion_table[___] + confusion_table[___])

# TNR
confusion_table[___]/(confusion_table[___] + confusion_table[___])

# Hint:
# ...
```

```{r}
#| label: tprTnrSolution
#| results: 'asis'

# TPR
confusion_table[1,1]/(confusion_table[1,1] + confusion_table[2,1])

# TNR
confusion_table[2,2]/(confusion_table[2,2] + confusion_table[1,2])
```

:::

Not bad. But remember that this is on the training data, without
cross-validation.

Let's have a closer look at `glmfit`. The `glmnet` package offers a diagnostic
plot that is worth looking at:

```{r}
#| label: plotglmfit
#| eval: true
#| echo: true
#| fig.width: 6
#| fig.height: 6
plot(glmfit, xvar = "norm", col = RColorBrewer::brewer.pal(12, "Set3"), lwd = sqrt(3))
```

What is the x-axis? What are the different lines? Check the ``plot.glmnet``
documentation and look at different `xvar` options.

In particular, we note that as the penalty $\lambda$ increases, the L1 Norm
of the coefficients ($\sum |\beta_i|$) shrinks. `glmnet` only shrinks the
coefficients corresponding to the variables and not the intercept, however for
simplicity our computations (and questions) below also include the intercept
(the first coefficient returned from `coef`).

For example as above let's see what fitted coefficients we got for
$\lambda = 0.04$:

```{r}
#| label: fitted_beta
#| eval: true
#| echo: true

# matrix of coefficients with given lambda
fitted_beta = coef(glmfit, s=0.04)
dim(fitted_beta)

# L1 norm of coefficients
sum(abs(fitted_beta))
```

Let's try with larger $\lambda$:

```{r}
#| label: coefLambda
#| eval: true
#| echo: true

# L1 norm of coefficients with given lambda
sum(abs(coef(glmfit, s=0.1)))
```

::: {.callout-note collapse="false"}

## Question

For how many different values of $\lambda$ did `glmnet` fit the model using settings as above?
Inspect the `glmfit` object.

```{r}
#| label: glmfitLambdaHint
#| eval: false
#| echo: !expr params$simplified
# Fill in the blanks to complete the code and answer the question above

______(glmfit$______)

# Hint:
# ...
```

```{r}
#| label: glmfitLambdaSolution
#| results: 'asis'
length(glmfit$lambda)
```
:::

::: {.callout-note collapse="false"}

## Question

For each of the values of $\lambda$ in the object above, calculate the L1 Norm (as we did above for two values of $\lambda$).
Which $\lambda$ most closely corresponds to a L1 norm 6000?

```{r}
#| label: lambda6kHint
#| eval: false
#| echo: !expr params$simplified
# Fill in the blanks to complete the code and answer the question above

# matrix of coefficients for all lambda
fitted_beta <- ____(______)
___(fitted_beta)

# L1 norm of coefficients for each lambda
L1Norm <- _______(___(fitted_beta))

# which L1 norm is clisest to 6000
idx_L1Norm_6k <- _________(___(L1Norm - 6000))
lambda_6k <- glmfit$______[idx_L1Norm_6k]
lambda_6k

# Hint:
# ...
```

```{r}
#| label: lambda6kSolution
#| results: 'asis'

# matrix of coefficients for all lambda
fitted_beta <- coef(glmfit)
dim(fitted_beta)

# L1 norm of coefficients for each lambda
L1Norm <- colSums(abs(fitted_beta))

# which L1 norm is clisest to 6000
idx_L1Norm_6k <- which.min(abs(L1Norm - 6000))
lambda_6k <- glmfit$lambda[idx_L1Norm_6k]
lambda_6k
```

:::

::: {.callout-note collapse="false"}

## Question

How many non-zero coefficients do you get for the $\lambda$ that you found in the previous question?

```{r}
#| label: lambda6kNonzeroHint
#| eval: false
#| echo: !expr params$simplified
# Fill in the blanks to complete the code and answer the question above

non0_coeff <- ___(____(______, s=_________)) > 0
___(non0_coeff)

# Hint:
# ...
```

```{r}
#| label: lambda6kNonzeroSolution
#| results: 'asis'
non0_coeff <- abs(coef(glmfit, s=lambda_6k)) > 0
sum(non0_coeff)
```

:::

To choose the best regularization parameter $\lambda$, we use cross-validation.
`cv.glmnet()` implements this.

```{r}
#| label: colonCV
#| eval: true
#| echo: true
set.seed(0xdada2)
cvglmfit = cv.glmnet(
  x = t(exprs(zellerNC)),
  y = factor(zellerNC$disease),
  family = "binomial"
)

# deviation for different lambdas
plot(cvglmfit)
```

We can access the optimal value with:

```{r}
#| label: lambda.min
#| eval: true
#| echo: true

cvglmfit$lambda.min

```

As this value results from finding a minimum in an estimated curve, it turns out
that it is often too small, i.e., that the implied penalization is too weak.
A heuristic recommended by the authors of the `glmnet` package is to use a
somewhat larger value instead, namely the largest value of $\lambda$ such that
the performance measure is within 1 standard error of the minimum.

```{r}
#| label: lambda.1se
#| eval: true
#| echo: true

cvglmfit$lambda.1se

```

::: {.callout-note collapse="false"}

## Question

How does the confusion table look like for $\lambda$ = `lambda.1se`?
Report the top left element of the confusion table
(i.e. number of correctly classified cancer samples).
Hint: Use function `predict`.

```{r}
#| label: preds.1seHint
#| eval: false
#| echo: !expr params$simplified
# Fill in the blanks to complete the code and answer the question above

# predictions
preds_1se <- _______(
  ______,
  newx = _(_____(zellerNC)),
  s = cvglmfit$__________,
  type = "class"
)

# confusion table
table(predicted = preds_1se, truth = zellerNC$_______)

# Hint:
# ...
```
```{r}
#| label: preds.1seSolution

# predictions
preds_1se <- predict(
  glmfit,
  newx = t(exprs(zellerNC)),
  s = cvglmfit$lambda.1se,
  type = "class"
)

# confusion table
table(predicted = preds_1se, truth = zellerNC$disease)
```

:::

::: {.callout-note collapse="false"}

## Question

What features drive the classification (at $\lambda$ = `lambda.1se` chosen by cross-validation with the 1 standard error rule)?
Report the top one (the one with the largest absolute value of its coefficient).

```{r}
#| label: idxLargestBetaHint
#| eval: false
#| echo: !expr params$simplified
# Fill in the blanks to complete the code and answer the question above

# coefficients with selected beta
coeffs <- ____(______, s=cvglmfit$__________)

# top coefficient
idx_largest_beta <- _________(___(as.numeric(coeffs)))
________(____(______))[idx_largest_beta]

# Hint:
# ...
```
```{r}
#| label: idxLargestBetaSolution
#| results: 'asis'
# coefficients with selected beta
coeffs <- coef(glmfit, s=cvglmfit$lambda.1se)
# top coefficient
idx_largest_beta <- which.max(abs(as.numeric(coeffs)))
rownames(coef(glmfit))[idx_largest_beta]
```

:::

## Method hacking

We encountered p-value hacking. A similar phenomenon exists in statistical
learning: given a dataset, we explore various different methods of preprocessing
(such as normalization, outlier detection, transformation, feature selection),
try out different machine learning algorithms and tune their parameters until
we are content with the result. The measured accuracy is likely to be too
optimistic, i.e., will not generalize to a new dataset. Embedding as many of
our methodical choices into a computational formalism and having an outer
cross-validation loop (not to be confused with the inner loop that does the
parameter tuning) will ameliorate the problem. But is unlikely to address it
completely, since not all our choices can be formalized.

The gold standard remains validation on truly unseen data. In addition, it is
never a bad thing if the classifier is not a black box but can be interpreted
in terms of domain knowledge. Finally, report not just summary statistics,
such as misclassification rates, but lay open the complete computational
workflow, so that anyone (including your future self) can convince themselves
of the robustness of the result or of the influence of the preprocessing,
model selection and tuning choices.

After this word of caution, have a look at the `caret` package. It contains
[a large number of machine learning methods](https://topepo.github.io/caret/available-models.html)
with an common interface.

```{r}
#| label: caret1
#| eval: true
#| echo: true
library("caret")
caretMethods = names(getModelInfo())
head(caretMethods, 8)
```

::: {.callout-note collapse="false"}

## Question

How many methods does `caret` currently include?

```{r}
#| label: caretMethodsHint
#| eval: false
#| echo: !expr params$simplified
# Fill in the blanks to complete the code and answer the question above

______(caretMethods)

# Hint:
# ...
```

```{r}
#| label: caretMethodsSolution
#| results: 'asis'
length(caretMethods)
```

:::

```{r}
#| label: sessionInfo
#| eval: !expr params$sessioninfo
#| echo: !expr params$sessioninfo
sessionInfo()
```
